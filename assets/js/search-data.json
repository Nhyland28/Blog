{
  
    
        "post0": {
            "title": "Tidy Data in Python",
            "content": "In this post we will be walking through the process of converting a messy Excel worksheet into tidy data. According to Hadley Wickham&#39;s excellent book, R For Data Science, tidy data follows three main principles: . Each variable must have its own column. | Each observation must have its own row. | Each value must have its own cell. | The initial work of organizing the data will pay dividends down the road as your data will be uniform and easier to work with. . For this tutorial we will be using the farm sector balance sheet provided by the United States Department of Agriculture (USDA). . . The USDA Excel shows a time series from 2014-2020 (with forecasted values for 2021). It is in a wide format with the variables (items in column &#39;A&#39;) representing rows instead of columns. Our goal will be to transform the dataframe into the below shape. We will have 5 columns: . Year | Balance item | Amount | Forecast (a boolean column indicating true if the amount is a forecast or historical data) | Report date | The last two columns (forecast and report date) may seem a little unnecessary. I included them because they will potentially be helpful keys if we were to include the report into a larger database. For instance, if we wanted to keep an archival database of all the farm sector balance sheets we could quickly identify observations with their report data. Additionally, I really like to indicate if the value is a forecast as it can lead into some interesting insights as to how their forecast changes over time and how it ends up performing to actual data. . The first step is to load our packages and then the Excel data into a dataframe. All we need is Numpy and Pandas. We will use Pandas&#39; read_excel() function to load the dataset. We&#39;ll pull data starting in row 3 of the Excel (we use header=2 here because read_excel() is zero-index while the spreadsheet is indexed at 1) and we&#39;ll read just for the first table (29 rows). Immediately after loading the data we will pull the date of the report into a variable that will be helpful once we create the report date column. . import numpy as np import pandas as pd . farm_raw = pd.read_excel(file_path, sheet_name=0, header=2, nrows=25) report_date = farm_raw.columns[4] . United States Unnamed: 1 Unnamed: 2 Data as of: 2021-09-02 00:00:00 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Change | NaN | . 1 | 2014 | 2015 | 2016.0 | 2017.0 | 2018.0 | 2019.0 | 2020.0 | 2021F | NaN | 2019 - 20 | 2020 - 21F | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | $ billion | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Percent | Percent | . 4 Cash income statement | | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . As you tell the above dataset is pretty messy. The first thing we will want to do is make the first row (remember it&#39;s zero-indexed) the column names. After that we can drop rows and columns that have NaNs in them as well as the two columns that contain year-over-year percent change (we will be creating a separate dataframe in a different blog that just measures this). . farm_raw.columns = farm_raw.iloc[1] # Remove rows that have NaNs in them farm_raw = farm_raw = farm_raw.dropna(axis=0, how=&#39;all&#39;) # Remove the one column that is an NaN ## The below code slices the DataFrame to include all columns do not include a null name farm_raw = farm_raw.loc[:, farm_raw.columns.notnull()] farm_raw = farm_raw.drop(columns=[&#39;2019 - 20&#39;, &#39;2020 - 21F&#39;]) . 1 2014 2015 2016.0 2017.0 2018.0 2019.0 2020.0 2021F . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 | 2014 | 2015 | 2016.000000 | 2017.000000 | 2018.000000 | 2019.000000 | 2020.000000 | 2021F | . 3 NaN | NaN | $ billion | NaN | NaN | NaN | NaN | NaN | NaN | . 4 Cash income statement | | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 a. Cash receipts | 423.971 | 377.432 | 358.481924 | 370.427294 | 371.182097 | 367.079638 | 357.160627 | 421.508 | . 6 Crops 1/ | 211.681 | 187.916 | 195.751261 | 194.867576 | 194.853082 | 191.630787 | 192.162673 | 230.054 | . 7 Animals and products | 212.29 | 189.516 | 162.730663 | 175.559718 | 176.329015 | 175.448851 | 164.997954 | 191.454 | . 8 b. Federal Government direct farm program pay... | 9.76684 | 10.8045 | 12.979677 | 11.531611 | 13.669010 | 22.447200 | 45.687724 | 28.0339 | . 9 c. Cash farm-related income 3/ | 36.5654 | 34.3769 | 27.891612 | 31.206105 | 29.125531 | 34.723812 | 34.314071 | 36.8362 | . 10 d. Gross cash income (a+b+c) | 470.303 | 422.613 | 399.353213 | 413.165009 | 413.976638 | 424.250650 | 437.162422 | 486.378 | . Let&#39;s set the first column as an index so it is a little easier to work with. Also, we can go ahead and delete the first 3 rows of the dataframe as they don&#39;t contain useful information. . ## We need to make the columns a list and then we can select the first column farm_raw = farm_raw.set_index(list(farm_raw.columns[0])) farm_raw = farm_raw.drop(index=farm_raw.index[:4]) . farm_raw.index . Index([&#39;a. Cash receipts &#39;, &#39; Crops 1/&#39;, &#39; Animals and products&#39;, &#39;b. Federal Government direct farm program payments 2/&#39;, &#39;c. Cash farm-related income 3/&#39;, &#39;d. Gross cash income (a+b+c)&#39;, &#39;e. Cash expenses 4/, 5/&#39;, &#39;f. Net cash income (d-e)&#39;, &#39;Farm income statement&#39;, &#39;g. Gross cash income (a+b+c)&#39;, &#39;h. Nonmoney income 6/ &#39;, &#39;i. Value of inventory adjustment &#39;, &#39;j. Total gross income (g+h+i)&#39;, &#39;k. Total expenses&#39;, &#39;l. Net farm income (j-k)&#39;], dtype=&#39;object&#39;, name=&#39; &#39;) . As we can see, the index items are messy with various letters preceeding the names and footnotes still present (e.g. &#39;1/&#39;). We will use a series of pandas string methods to clean up the that text column. . farm_raw.index = farm_raw.index.str.lower() # Using regular expressions to remove the lower-case row labels # E.g. the &#39;a.&#39; in &#39;a. Cash receipts&#39; ## Since there is no str.remove function we will just replace the pattern we want to drop with an empty string farm_raw.index = farm_raw.index.str.replace(r&#39;[a-z] .&#39;, &#39;&#39;) # Remove all the parentheses and the chartacters within them # E.g. the &#39;(a+b+c)&#39; in &#39;g. Gross cash income (a+b+c)&#39; farm_raw.index = farm_raw.index.str.replace(r&#39; (([^ )]+) )&#39;, &#39;&#39;) # Remove all the footnote labels # E.g. the &#39;2/&#39; in &#39;Federal Government direct farm program payments&#39; farm_raw.index = farm_raw.index.str.replace(r&#39;[1-9]/&#39;, &#39;&#39;) # Remove all commas farm_raw.index = farm_raw.index.str.replace(&#39;,&#39;, &#39;&#39;) # Remove all the white space before and after the strong farm_raw.index = farm_raw.index.str.strip() # Replace spaces with underscores farm_raw.index = farm_raw.index.str.replace(&#39; &#39;, &#39;_&#39;) . With the columns cleaned up we can put the data into long format. The first thing we will do is transpose our dataframe (make the columns the rows and the rows the columns). . ## This will allow us to melt the data frame (next step) easier farm_raw = farm_raw.transpose().reset_index() . 1 cash_receipts crops animals_and_products federal_government_direct_farm_program_payments cash_farm-related_income gross_cash_income cash_expenses net_cash_income farm_income_statement gross_cash_income nonmoney_income value_of_inventory_adjustment total_gross_income total_expenses net_farm_income . 0 2014 | 423.971 | 211.681 | 212.29 | 9.76684 | 36.5654 | 470.303 | 338.998 | 131.306 | NaN | 470.303 | 16.8919 | -3.907 | 483.288 | 391.05 | 92.238 | . 1 2015 | 377.432 | 187.916 | 189.516 | 10.8045 | 34.3769 | 422.613 | 315.829 | 106.785 | NaN | 422.613 | 17.7622 | 0.419605 | 440.795 | 359.131 | 81.664 | . 2 2016 | 358.482 | 195.751 | 162.731 | 12.9797 | 27.8916 | 399.353 | 303.784 | 95.5696 | NaN | 399.353 | 17.1479 | -4.24842 | 412.253 | 349.938 | 62.3145 | . 3 2017 | 370.427 | 194.868 | 175.56 | 11.5316 | 31.2061 | 413.165 | 311.892 | 101.273 | NaN | 413.165 | 18.2841 | -6.04901 | 425.4 | 350.285 | 75.1147 | . 4 2018 | 371.182 | 194.853 | 176.329 | 13.669 | 29.1255 | 413.977 | 311.398 | 102.579 | NaN | 413.977 | 19.1378 | -8.23461 | 424.88 | 343.815 | 81.0648 | . Next we will melt the dataframe. This powerful function (pd.melt()) makes our current wide dataframe into a long dataframe. The documentation describes it as this: . &quot;one or more columns are identifier variables (for our case the year column), while all other columns, considered measured variables are &#39;unpivoted&#39; to the row axis, leaving just two non-identifier columns, &#39;variable&#39; (measurement, e.g. &#39;cash_receipts&#39;) and &#39;value&#39; (the balance values... the numbers).&quot; . farm_raw = pd.melt(frame=farm_raw, id_vars=[1]) . 1 value . 0 2014 | cash_receipts | 423.971 | . 1 2015 | cash_receipts | 377.432 | . 2 2016 | cash_receipts | 358.482 | . 3 2017 | cash_receipts | 370.427 | . 4 2018 | cash_receipts | 371.182 | . 5 2019 | cash_receipts | 367.08 | . 6 2020 | cash_receipts | 357.161 | . 7 2021F | cash_receipts | 421.508 | . 8 2014 | crops | 211.681 | . 9 2015 | crops | 187.916 | . 10 2016 | crops | 195.751 | . 11 2017 | crops | 194.868 | . 12 2018 | crops | 194.853 | . 13 2019 | crops | 191.631 | . 14 2020 | crops | 192.163 | . 15 2021F | crops | 230.054 | . 16 2014 | animals_and_products | 212.29 | . 17 2015 | animals_and_products | 189.516 | . 18 2016 | animals_and_products | 162.731 | . 19 2017 | animals_and_products | 175.56 | . As you can see, our dataframe now consists of just three columns. We could have kept it unmelted and it would have technically been tidy. Each row was an observation and each column was a separate variable. However, we want to add two more columns for each observation - if it was a forecast and the date of the report it was associated with. . First let&#39;s rename those columns that we already have. . farm_raw.columns = [&#39;year&#39;, &#39;balance_item&#39;,&#39;value&#39;] . Next let&#39;s add a boolean column (true or false) to show whether the observation was a forecast or not. The Excel indicated forecasts by adding an &quot;F&quot; at the end of the date (2021F). We will use Numpy&#39;s where function to indicate False for columns that just have 4 numbers and True for everything else (columns with an &quot;F&quot; for forecast). . farm_raw[&#39;forecast&#39;] = np.where(farm_raw[&#39;year&#39;].str.contains(&#39;0000&#39;), False, True) . Now we can add a column showing the report date (remember we pulled this earlier from the spreadsheet and saved it into a variable report_date). . farm_raw[&#39;report_date&#39;] = report_date . year balance_item value forecast report_date . 0 2014 | cash_receipts | 423.971 | False | 2021-09-02 | . 1 2015 | cash_receipts | 377.432 | False | 2021-09-02 | . 2 2016 | cash_receipts | 358.482 | False | 2021-09-02 | . 3 2017 | cash_receipts | 370.427 | False | 2021-09-02 | . 4 2018 | cash_receipts | 371.182 | False | 2021-09-02 | . We&#39;re almost there! Lets dig a little deeper into our columns (variables) and see what data types they are. . farm_raw.dtypes . year object balance_item object value object forecast bool report_date datetime64[ns] dtype: object . Good thing we checked! Both our year column and value column are objects when we would want them to be integers and floats, respectively. This makes sense if you remember our original data set (especially since we never manually assigned the columns data types - a good habit I should admittedly get better with). There was likely some strings and floasts in the columns that the year and balance_item columns are derived from so they automatically got converted into objects. Luckily this is an easy fix. . For the year column we have to get rid of the &quot;F&quot; for forecasted values, make sure its only 4 digits, and then convert it to an integer type. . farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].astype(str) # Remove all non-digits (D). This is meant to drop the &#39;F&#39; farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].str.replace(&#39; D&#39;,&#39;&#39;) # Only include the 4 numbers for a year farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].str.slice(stop=4) # Convert the column to an integer farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].astype(int) . The value column is much easier. We can just convert it to a float using the above .astype() function. . farm_raw.value = farm_raw.value.astype(float) . farm_raw.dtypes . year int32 balance_item object value float64 forecast bool report_date datetime64[ns] dtype: object . Much better! All our values are now datatypes we would expect. And with that, we&#39;ve cleaned the data! There&#39;s still much more we can do. We can easily navigate and filter this dataframe with Pandas, add on previous reports from USDA, and create graphics. In the future I&#39;ll have a blog post that will show how we can easily create a corresponding dataframe that looks represents the data in year-over-year percent change - a valuable way to look at economic data. . As a final step let&#39;s make the farm_raw into just farm and then take a look at our clean and tidy dataset! . farm = farm_raw . year balance_item value forecast report_date . 0 2014 | cash_receipts | 423.970804 | False | 2021-09-02 | . 1 2015 | cash_receipts | 377.432066 | False | 2021-09-02 | . 2 2016 | cash_receipts | 358.481924 | False | 2021-09-02 | . 3 2017 | cash_receipts | 370.427294 | False | 2021-09-02 | . 4 2018 | cash_receipts | 371.182097 | False | 2021-09-02 | . 5 2019 | cash_receipts | 367.079638 | False | 2021-09-02 | . 6 2020 | cash_receipts | 357.160627 | False | 2021-09-02 | . 7 2021 | cash_receipts | 421.508110 | True | 2021-09-02 | . 8 2014 | crops | 211.680565 | False | 2021-09-02 | . 9 2015 | crops | 187.916477 | False | 2021-09-02 | . 10 2016 | crops | 195.751261 | False | 2021-09-02 | . 11 2017 | crops | 194.867576 | False | 2021-09-02 | . 12 2018 | crops | 194.853082 | False | 2021-09-02 | . 13 2019 | crops | 191.630787 | False | 2021-09-02 | . 14 2020 | crops | 192.162673 | False | 2021-09-02 | . 15 2021 | crops | 230.053695 | True | 2021-09-02 | . 16 2014 | animals_and_products | 212.290239 | False | 2021-09-02 | . 17 2015 | animals_and_products | 189.515589 | False | 2021-09-02 | . 18 2016 | animals_and_products | 162.730663 | False | 2021-09-02 | . 19 2017 | animals_and_products | 175.559718 | False | 2021-09-02 | .",
            "url": "https://nhyland28.github.io/Blog/data%20cleaning/pandas/2021/10/17/Tidy-Data-in-Python.html",
            "relUrl": "/data%20cleaning/pandas/2021/10/17/Tidy-Data-in-Python.html",
            "date": " • Oct 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nick Hyland and I work as senior data analyst for a small consulting firm that studies energy-related issues. This blog is intended to document my continuing journey into data analysis and artificial intelligence. It will include projects that focus on data analysis and cleaning primarily using Python and Pandas. I also will include projects and thoughts on artificial intelligence and machine learning as I work through fast.ai’s deep learning classes. I hope along the way some of my posts will help some other people entering the field! . I started programming in college at American University where I majored in International Studies (basically a branch of political science). While my degree is in the social sciences I learned through computer science, statistics, and research classes that I really loved working with data and writing programs. Since college I have continued to learn about data science and currently work full time as a data analyst. My work days are spent cleaning data, making dashboards and automated reports, creating and analyzing forecasts, and giving clients market insights in the form of data visualizations and written content. . Currently I am based in Denver, Colorado where I love to hike and ski. I’m also a big sports fan. Lets go Mets, Jets, and Syracuse Orange! .",
          "url": "https://nhyland28.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhyland28.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}