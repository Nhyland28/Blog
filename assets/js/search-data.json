{
  
    
        "post0": {
            "title": "Project Euler Problems 1 and 2",
            "content": "In order to stay fresh with general programming skills I am going to attempt various Project Euler problems and walk through my solutions. For those of you that do not know Project Euler it &quot;is a series of challenging mathematical/computer programming problems.&quot; Simply put, it is a great way to practice your computer programming skills. . In this blog I&#39;m going to work on Project Euler problems 1 and 2. . Problem 1: Multiples of 3 or 5 . If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. . Find the sum of all the multiples of 3 or 5 below 1000. . Breaking down the problem: . It&#39;s pretty straightforward. We want all the natural numbers (so positive integers... no decimals) under 1,000 that are multiples of 3 or 5. A multiple is simply &quot;a number that can be divided by another number without a remainder.&quot; So, multiples of 3 include 3, 6, 9, 12... . Then we just want to take the sum of all those numbers. . I am going to create a function with 2 arguments: . A list of the numbers we want multiples of (so in this case 3 and 5) | The max number we want (in this case 1000) | The function will then do the following steps: . Initialize a list (sum_list). This is where we will store the multiples of the numbers we are interested in. For the case of this problem, I mean all the multiples of 3 and 5. | Loop through 1 and the max_number (1000 for this particular problem). We&#39;ll call this number i. | Loop through each of the dividers (e.g. 3 and 5) and use the modulo operator, %, to determine if the number, i, is even or odd. The modulo operator returns the remainder of a division problem (e.g. 4 % 1 = 1). | If the number i has no remainder when divided by a divider we will .append (or add) it to the sum_list we created earlier. | One problem this creates is there could be duplicates. For example, 15 would show up twice in sum_list as both 3 and 5 go into it. We can solve this by removing duplicates in the sum_list. The set() function is any easy way to do this. The function converts the object into a Python set (one of the 4 major Python data classes along with lists, tuples, and dictionaries). Sets &quot;do not allow duplicate values&quot; so the use of list(set(sum_list)) will convert the sum_list into a set, effectively dropping duplicate values, then converting it back into a list. | The last step is to use the sum() function to calculate the sum of all the multiples stored in sum_list. | def euler_1(dividers, max_number): sum_list = [] for i in range(1, max_number): for div in dividers: if i % div == 0: sum_list.append(i) sum_list = list(set(sum_list)) return(sum(sum_list)) . Running our function with the arguments from the question (multiples of 3 and 5 up to 1000) we get an answer of 233,168. . euler_1([3,5], 1000) . 233168 . Problem 1: Even Fibonacci numbers . Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be: . 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... . By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms. . Breaking down the problem: . This problem will involve 3 parts: . Creating a Fibonacci sequence that stops right before 4,000,000 | Sorting out only the even numbers | Finding the sum of all those even numbers | Similar to the first Euler problem, I&#39;m going to create a function that takes two arguments: . The maximum number for the Fibonacci sequence (4,000,000) in this case | And, a boolean variable determining if we want to sort even numbers | The function will then do the following steps: . Create a variable called modulo_div that will allow us to toggle whether we want to sum the even-valued terms (such as in this problem) or odd-valued terms | Create a list, fibonacci, with the first two terms of the Fibonacci sequence (1 and 2) | Create a variable, i, which will serve as an iterator | Put i into a while loop. Then, add together the last two numbers of the fibonacci list and make that sum the value of i. For example, on the first iteration we will sum 1 and 2 making i equal to 3. | As long as the value of i is less than 4,000,000 add it to the end of fibonacci using append(). If the value of i exceeds 4,000,000 do not add it to the list and break the loop. | Once the loop is broken we create a new list, fibonacci_portion. Use a Python list comprehension to go through the fibonacci list and only add even numbers. It will be the same method we used to gather odd numbers in the previous problem (using the modulo operator). | Finally, return the sum of the fibonacci_portion list (so all even numbers in the Fibonacci sequence up to 4,000,000) | def euler_2(max_number, even = True): if even == True: modulo_div = 2 else: modulo_div = 1 fibonacci = [1,2] i = 1 while i &gt; 0: i = sum(fibonacci[-2:]) if i &gt; 4000000: break fibonacci.append(i) fibonacci_portion = [j for j in fibonacci if j % modulo_div == 0] return(sum(fibonacci_portion)) . Running our function with the arguments from the question (a maximum of 4,000,000 and looking at even numbers) we get an answer of 4,613,732. . euler_2(4000000, even = True) . 4613732 .",
            "url": "https://nhyland28.github.io/Blog/euler/programming/2021/11/03/Project_Euler_Problems_1_and_2.html",
            "relUrl": "/euler/programming/2021/11/03/Project_Euler_Problems_1_and_2.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tidy Data in Python",
            "content": "In this post we will be walking through the process of converting a messy Excel worksheet into tidy data. According to Hadley Wickham&#39;s excellent book, R For Data Science, tidy data follows three main principles: . Each variable must have its own column. | Each observation must have its own row. | Each value must have its own cell. | The initial work of organizing the data will pay dividends down the road as your data will be uniform and easier to work with. . For this tutorial we will be using the farm sector balance sheet provided by the United States Department of Agriculture (USDA). . . The USDA Excel shows a time series from 2014-2020 (with forecasted values for 2021). It is in a wide format with the variables (items in column &#39;A&#39;) representing rows instead of columns. Our goal will be to transform the dataframe into the below shape. We will have 5 columns: . Year | Balance item | Amount | Forecast (a boolean column indicating true if the amount is a forecast or historical data) | Report date | The last two columns (forecast and report date) may seem a little unnecessary. I included them because they will potentially be helpful keys if we were to include the report into a larger database. For instance, if we wanted to keep an archival database of all the farm sector balance sheets we could quickly identify observations with their report data. Additionally, I really like to indicate if the value is a forecast as it can lead into some interesting insights as to how their forecast changes over time and how it ends up performing to actual data. . The first step is to load our packages and then the Excel data into a dataframe. All we need is Numpy and Pandas. We will use Pandas&#39; read_excel() function to load the dataset. We&#39;ll pull data starting in row 3 of the Excel (we use header=2 here because read_excel() is zero-index while the spreadsheet is indexed at 1) and we&#39;ll read just for the first table (29 rows). Immediately after loading the data we will pull the date of the report into a variable that will be helpful once we create the report date column. . import numpy as np import pandas as pd . farm_raw = pd.read_excel(file_path, sheet_name=0, header=2, nrows=25) report_date = farm_raw.columns[4] . United States Unnamed: 1 Unnamed: 2 Data as of: 2021-09-02 00:00:00 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Change | NaN | . 1 | 2014 | 2015 | 2016.0 | 2017.0 | 2018.0 | 2019.0 | 2020.0 | 2021F | NaN | 2019 - 20 | 2020 - 21F | . 2 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | $ billion | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Percent | Percent | . 4 Cash income statement | | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . As you tell the above dataset is pretty messy. The first thing we will want to do is make the first row (remember it&#39;s zero-indexed) the column names. After that we can drop rows and columns that have NaNs in them as well as the two columns that contain year-over-year percent change (we will be creating a separate dataframe in a different blog that just measures this). . farm_raw.columns = farm_raw.iloc[1] # Remove rows that have NaNs in them farm_raw = farm_raw = farm_raw.dropna(axis=0, how=&#39;all&#39;) # Remove the one column that is an NaN ## The below code slices the DataFrame to include all columns do not include a null name farm_raw = farm_raw.loc[:, farm_raw.columns.notnull()] farm_raw = farm_raw.drop(columns=[&#39;2019 - 20&#39;, &#39;2020 - 21F&#39;]) . 1 2014 2015 2016.0 2017.0 2018.0 2019.0 2020.0 2021F . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 | 2014 | 2015 | 2016.000000 | 2017.000000 | 2018.000000 | 2019.000000 | 2020.000000 | 2021F | . 3 NaN | NaN | $ billion | NaN | NaN | NaN | NaN | NaN | NaN | . 4 Cash income statement | | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 a. Cash receipts | 423.971 | 377.432 | 358.481924 | 370.427294 | 371.182097 | 367.079638 | 357.160627 | 421.508 | . 6 Crops 1/ | 211.681 | 187.916 | 195.751261 | 194.867576 | 194.853082 | 191.630787 | 192.162673 | 230.054 | . 7 Animals and products | 212.29 | 189.516 | 162.730663 | 175.559718 | 176.329015 | 175.448851 | 164.997954 | 191.454 | . 8 b. Federal Government direct farm program pay... | 9.76684 | 10.8045 | 12.979677 | 11.531611 | 13.669010 | 22.447200 | 45.687724 | 28.0339 | . 9 c. Cash farm-related income 3/ | 36.5654 | 34.3769 | 27.891612 | 31.206105 | 29.125531 | 34.723812 | 34.314071 | 36.8362 | . 10 d. Gross cash income (a+b+c) | 470.303 | 422.613 | 399.353213 | 413.165009 | 413.976638 | 424.250650 | 437.162422 | 486.378 | . Let&#39;s set the first column as an index so it is a little easier to work with. Also, we can go ahead and delete the first 3 rows of the dataframe as they don&#39;t contain useful information. . ## We need to make the columns a list and then we can select the first column farm_raw = farm_raw.set_index(list(farm_raw.columns[0])) farm_raw = farm_raw.drop(index=farm_raw.index[:4]) . farm_raw.index . Index([&#39;a. Cash receipts &#39;, &#39; Crops 1/&#39;, &#39; Animals and products&#39;, &#39;b. Federal Government direct farm program payments 2/&#39;, &#39;c. Cash farm-related income 3/&#39;, &#39;d. Gross cash income (a+b+c)&#39;, &#39;e. Cash expenses 4/, 5/&#39;, &#39;f. Net cash income (d-e)&#39;, &#39;Farm income statement&#39;, &#39;g. Gross cash income (a+b+c)&#39;, &#39;h. Nonmoney income 6/ &#39;, &#39;i. Value of inventory adjustment &#39;, &#39;j. Total gross income (g+h+i)&#39;, &#39;k. Total expenses&#39;, &#39;l. Net farm income (j-k)&#39;], dtype=&#39;object&#39;, name=&#39; &#39;) . As we can see, the index items are messy with various letters preceeding the names and footnotes still present (e.g. &#39;1/&#39;). We will use a series of pandas string methods to clean up the that text column. . farm_raw.index = farm_raw.index.str.lower() # Using regular expressions to remove the lower-case row labels # E.g. the &#39;a.&#39; in &#39;a. Cash receipts&#39; ## Since there is no str.remove function we will just replace the pattern we want to drop with an empty string farm_raw.index = farm_raw.index.str.replace(r&#39;[a-z] .&#39;, &#39;&#39;) # Remove all the parentheses and the chartacters within them # E.g. the &#39;(a+b+c)&#39; in &#39;g. Gross cash income (a+b+c)&#39; farm_raw.index = farm_raw.index.str.replace(r&#39; (([^ )]+) )&#39;, &#39;&#39;) # Remove all the footnote labels # E.g. the &#39;2/&#39; in &#39;Federal Government direct farm program payments&#39; farm_raw.index = farm_raw.index.str.replace(r&#39;[1-9]/&#39;, &#39;&#39;) # Remove all commas farm_raw.index = farm_raw.index.str.replace(&#39;,&#39;, &#39;&#39;) # Remove all the white space before and after the strong farm_raw.index = farm_raw.index.str.strip() # Replace spaces with underscores farm_raw.index = farm_raw.index.str.replace(&#39; &#39;, &#39;_&#39;) . With the columns cleaned up we can put the data into long format. The first thing we will do is transpose our dataframe (make the columns the rows and the rows the columns). . ## This will allow us to melt the data frame (next step) easier farm_raw = farm_raw.transpose().reset_index() . 1 cash_receipts crops animals_and_products federal_government_direct_farm_program_payments cash_farm-related_income gross_cash_income cash_expenses net_cash_income farm_income_statement gross_cash_income nonmoney_income value_of_inventory_adjustment total_gross_income total_expenses net_farm_income . 0 2014 | 423.971 | 211.681 | 212.29 | 9.76684 | 36.5654 | 470.303 | 338.998 | 131.306 | NaN | 470.303 | 16.8919 | -3.907 | 483.288 | 391.05 | 92.238 | . 1 2015 | 377.432 | 187.916 | 189.516 | 10.8045 | 34.3769 | 422.613 | 315.829 | 106.785 | NaN | 422.613 | 17.7622 | 0.419605 | 440.795 | 359.131 | 81.664 | . 2 2016 | 358.482 | 195.751 | 162.731 | 12.9797 | 27.8916 | 399.353 | 303.784 | 95.5696 | NaN | 399.353 | 17.1479 | -4.24842 | 412.253 | 349.938 | 62.3145 | . 3 2017 | 370.427 | 194.868 | 175.56 | 11.5316 | 31.2061 | 413.165 | 311.892 | 101.273 | NaN | 413.165 | 18.2841 | -6.04901 | 425.4 | 350.285 | 75.1147 | . 4 2018 | 371.182 | 194.853 | 176.329 | 13.669 | 29.1255 | 413.977 | 311.398 | 102.579 | NaN | 413.977 | 19.1378 | -8.23461 | 424.88 | 343.815 | 81.0648 | . Next we will melt the dataframe. This powerful function (pd.melt()) makes our current wide dataframe into a long dataframe. The documentation describes it as this: . &quot;one or more columns are identifier variables (for our case the year column), while all other columns, considered measured variables are &#39;unpivoted&#39; to the row axis, leaving just two non-identifier columns, &#39;variable&#39; (measurement, e.g. &#39;cash_receipts&#39;) and &#39;value&#39; (the balance values... the numbers).&quot; . farm_raw = pd.melt(frame=farm_raw, id_vars=[1]) . 1 value . 0 2014 | cash_receipts | 423.971 | . 1 2015 | cash_receipts | 377.432 | . 2 2016 | cash_receipts | 358.482 | . 3 2017 | cash_receipts | 370.427 | . 4 2018 | cash_receipts | 371.182 | . 5 2019 | cash_receipts | 367.08 | . 6 2020 | cash_receipts | 357.161 | . 7 2021F | cash_receipts | 421.508 | . 8 2014 | crops | 211.681 | . 9 2015 | crops | 187.916 | . 10 2016 | crops | 195.751 | . 11 2017 | crops | 194.868 | . 12 2018 | crops | 194.853 | . 13 2019 | crops | 191.631 | . 14 2020 | crops | 192.163 | . 15 2021F | crops | 230.054 | . 16 2014 | animals_and_products | 212.29 | . 17 2015 | animals_and_products | 189.516 | . 18 2016 | animals_and_products | 162.731 | . 19 2017 | animals_and_products | 175.56 | . As you can see, our dataframe now consists of just three columns. We could have kept it unmelted and it would have technically been tidy. Each row was an observation and each column was a separate variable. However, we want to add two more columns for each observation - if it was a forecast and the date of the report it was associated with. . First let&#39;s rename those columns that we already have. . farm_raw.columns = [&#39;year&#39;, &#39;balance_item&#39;,&#39;value&#39;] . Next let&#39;s add a boolean column (true or false) to show whether the observation was a forecast or not. The Excel indicated forecasts by adding an &quot;F&quot; at the end of the date (2021F). We will use Numpy&#39;s where function to indicate False for columns that just have 4 numbers and True for everything else (columns with an &quot;F&quot; for forecast). . farm_raw[&#39;forecast&#39;] = np.where(farm_raw[&#39;year&#39;].str.contains(&#39;0000&#39;), False, True) . Now we can add a column showing the report date (remember we pulled this earlier from the spreadsheet and saved it into a variable report_date). . farm_raw[&#39;report_date&#39;] = report_date . year balance_item value forecast report_date . 0 2014 | cash_receipts | 423.971 | False | 2021-09-02 | . 1 2015 | cash_receipts | 377.432 | False | 2021-09-02 | . 2 2016 | cash_receipts | 358.482 | False | 2021-09-02 | . 3 2017 | cash_receipts | 370.427 | False | 2021-09-02 | . 4 2018 | cash_receipts | 371.182 | False | 2021-09-02 | . We&#39;re almost there! Lets dig a little deeper into our columns (variables) and see what data types they are. . farm_raw.dtypes . year object balance_item object value object forecast bool report_date datetime64[ns] dtype: object . Good thing we checked! Both our year column and value column are objects when we would want them to be integers and floats, respectively. This makes sense if you remember our original data set (especially since we never manually assigned the columns data types - a good habit I should admittedly get better with). There was likely some strings and floasts in the columns that the year and balance_item columns are derived from so they automatically got converted into objects. Luckily this is an easy fix. . For the year column we have to get rid of the &quot;F&quot; for forecasted values, make sure its only 4 digits, and then convert it to an integer type. . farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].astype(str) # Remove all non-digits (D). This is meant to drop the &#39;F&#39; farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].str.replace(&#39; D&#39;,&#39;&#39;) # Only include the 4 numbers for a year farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].str.slice(stop=4) # Convert the column to an integer farm_raw[&#39;year&#39;] = farm_raw[&#39;year&#39;].astype(int) . The value column is much easier. We can just convert it to a float using the above .astype() function. . farm_raw.value = farm_raw.value.astype(float) . farm_raw.dtypes . year int32 balance_item object value float64 forecast bool report_date datetime64[ns] dtype: object . Much better! All our values are now datatypes we would expect. And with that, we&#39;ve cleaned the data! There&#39;s still much more we can do. We can easily navigate and filter this dataframe with Pandas, add on previous reports from USDA, and create graphics. In the future I&#39;ll have a blog post that will show how we can easily create a corresponding dataframe that looks represents the data in year-over-year percent change - a valuable way to look at economic data. . As a final step let&#39;s make the farm_raw into just farm and then take a look at our clean and tidy dataset! . farm = farm_raw . year balance_item value forecast report_date . 0 2014 | cash_receipts | 423.970804 | False | 2021-09-02 | . 1 2015 | cash_receipts | 377.432066 | False | 2021-09-02 | . 2 2016 | cash_receipts | 358.481924 | False | 2021-09-02 | . 3 2017 | cash_receipts | 370.427294 | False | 2021-09-02 | . 4 2018 | cash_receipts | 371.182097 | False | 2021-09-02 | . 5 2019 | cash_receipts | 367.079638 | False | 2021-09-02 | . 6 2020 | cash_receipts | 357.160627 | False | 2021-09-02 | . 7 2021 | cash_receipts | 421.508110 | True | 2021-09-02 | . 8 2014 | crops | 211.680565 | False | 2021-09-02 | . 9 2015 | crops | 187.916477 | False | 2021-09-02 | . 10 2016 | crops | 195.751261 | False | 2021-09-02 | . 11 2017 | crops | 194.867576 | False | 2021-09-02 | . 12 2018 | crops | 194.853082 | False | 2021-09-02 | . 13 2019 | crops | 191.630787 | False | 2021-09-02 | . 14 2020 | crops | 192.162673 | False | 2021-09-02 | . 15 2021 | crops | 230.053695 | True | 2021-09-02 | . 16 2014 | animals_and_products | 212.290239 | False | 2021-09-02 | . 17 2015 | animals_and_products | 189.515589 | False | 2021-09-02 | . 18 2016 | animals_and_products | 162.730663 | False | 2021-09-02 | . 19 2017 | animals_and_products | 175.559718 | False | 2021-09-02 | .",
            "url": "https://nhyland28.github.io/Blog/data%20cleaning/pandas/2021/10/17/Tidy-Data-in-Python.html",
            "relUrl": "/data%20cleaning/pandas/2021/10/17/Tidy-Data-in-Python.html",
            "date": " • Oct 17, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Nick Hyland and I work as senior data analyst for a small consulting firm that studies energy-related issues. This blog is intended to document my continuing journey into data analysis and artificial intelligence. It will include projects that focus on data analysis and cleaning primarily using Python and Pandas. I also will include projects and thoughts on artificial intelligence and machine learning as I work through fast.ai’s deep learning classes. I hope along the way some of my posts will help some other people entering the field! . I started programming in college at American University where I majored in International Studies (basically a branch of political science). While my degree is in the social sciences I learned through computer science, statistics, and research classes that I really loved working with data and writing programs. Since college I have continued to learn about data science and currently work full time as a data analyst. My work days are spent cleaning data, making dashboards and automated reports, creating and analyzing forecasts, and giving clients market insights in the form of data visualizations and written content. . Currently I am based in Denver, Colorado where I love to hike and ski. I’m also a big sports fan. Lets go Mets, Jets, and Syracuse Orange! .",
          "url": "https://nhyland28.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhyland28.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}